# -*- coding: utf-8 -*-
"""kaggle_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R1Vwl_z8JT_sphlPiMaxjkewwG3zHrN7
"""

!cd /root

!mkdir -p /root/.kaggle
!cp kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!ls /root/.kaggle

!kaggle datasets list

!kaggle datasets download -d kazanova/sentiment140

!unzip sentiment140.zip

import pandas as pd

df = pd.read_csv("training.1600000.processed.noemoticon.csv",
                 encoding='latin-1', header=None)

df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']
data = df[['text', 'target']].copy() # Create a copy to avoid the warning
data.loc[:, 'target'] = data['target'].map({0: 'negative', 4: 'positive'})
data.loc[:, 'target'] = data['target'].astype('category')

data = data.sample(frac=1, random_state=42).reset_index(drop=True)

print(data.head(20))

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
print(tokenizer.tokenize("Hello, I'm a single sentence!"))
print(tokenizer.vocab_size)

from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

class thisDataset(Dataset):
  def __init__(self, df, tokenizer):
      self.df = df
      self.tokenizer = tokenizer
      # Define label mapping for numerical conversion
      self.label_map = {'negative': 0, 'positive': 1}

  def __len__(self):
      return len(self.df)

  def __getitem__(self, idx):
      text = self.df['text'].iloc[idx]
      # Tokenize without padding or truncation here; collate_fn will handle it per batch
      # Return tokenized input_ids and attention_mask as lists/tensors to be processed by collate_fn
      encoded = self.tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)
      input_ids = encoded['input_ids'].squeeze(0)
      attention_mask = encoded['attention_mask'].squeeze(0)

      # Get the label and convert it to a numerical tensor (0 for negative, 1 for positive)
      label = torch.tensor(self.label_map[self.df['target'].iloc[idx]], dtype=torch.long)

      return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}

testData = data.iloc[140000:141000, :]
data = data.iloc[:10000, :]
print(f"TEST LABEL DISTRIBUTION: {testData['target'].value_counts()}") # check label distribution
print(f"TRAIN LABEL DISTRIBUTION: {data['target'].value_counts()}") # check label distribution

dataset = thisDataset(data, tokenizer)
print(f"Dataset size: {len(dataset)}")

dataLoader = DataLoader(dataset, batch_size=16, shuffle=True)

for i, item in enumerate(dataLoader):
  print(item)
  if i == 1:
    break

import torch
from torch import nn
from transformers import AutoModel
from torch.utils.data import DataLoader # Ensure DataLoader is imported if not already in this cell
from transformers import DataCollatorWithPadding
from torch.cuda.amp import GradScaler, autocast
import time

scaler = GradScaler()
collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)


class sentiment1(nn.Module):
  def __init__(self, num_labels, dropout=0.3):
    super().__init__()
    self.bert = AutoModel.from_pretrained("bert-base-cased")
    self.drop = nn.Dropout(p=dropout)
    self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
    self.classifier = nn.utils.weight_norm(self.classifier)

  def forward(self, input_ids, attention_mask):
    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
    CLS = outputs.last_hidden_state[:, 0, :]
    CLS = self.drop(CLS)
    logits = self.classifier(CLS)
    return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = sentiment1(num_labels=2).to(device)

BATCH_SIZE=32
EPOCHS=2

train_loader1 = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

for epoch in range(EPOCHS):
  model.train()
  totalError = 0
  total_samples=0
  for i, batch in enumerate(train_loader1):
    torch.cuda.synchronize()
    start = time.time()

    print("running") if i % 100 == 0 else None
    optimizer.zero_grad()

    input_ids = batch['input_ids'].to(device, non_blocking=True)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    with torch.amp.autocast('cuda'):
      outputs = model(input_ids, attention_mask)
      loss = criterion(outputs, labels)
      print(f'error: {loss.item()}')
      scaler.scale(loss).backward()
      scaler.step(optimizer)
      scaler.update()
    torch.cuda.synchronize()
    batch_time = time.time() - start

    total_samples += BATCH_SIZE

    if (i+1) % 10 == 0:
        print(f"batch {i+1}, batch_time={batch_time:.3f}s, samples/sec={BATCH_SIZE/batch_time:.1f}")

    totalError += loss.item()
  print(f"epoch: {epoch} | Loss: {totalError / len(train_loader1)}")

torch.save(model.state_dict(), "/content/model.pth")

import os
print(os.path.getsize("model.pth"))
model.load_state_dict(torch.load("/content/model.pth"))

test_dataset = thisDataset(testData, tokenizer)
print(len(testData))
print(testData['target'].value_counts()) # check label distribution
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
print(f"length of test loader: {len(test_loader)} batches @ 32 samples/batch") # 32 batches per epoch for 32 samples/batch

def test(model, test_loader):
  model.eval()
  total_correct = 0
  total_samples = len(testData)
  total_loss = 0
  all_probs = []

  with torch.inference_mode():

    for i, batch in enumerate(test_loader):
      input_ids = batch['input_ids'].to(device, non_blocking=True)
      attention_mask = batch['attention_mask'].to(device, non_blocking=True)
      labels = batch['labels'].to(device)

      outputs = model(input_ids, attention_mask)
      loss = criterion(outputs, labels)
      total_loss += loss.item()

      probabilities = torch.softmax(outputs, dim=1)
      if i%100 == 0:
        print(f"running: {i}")

      predicted_labels = torch.argmax(probabilities, dim=1)
      total_correct += (predicted_labels == labels).sum().item()

  test_accuracy = total_correct / total_samples
  average_loss = total_loss / total_samples

  print(f"test accuracy: {test_accuracy}")
  print(f"average loss: {average_loss}")

  return test_accuracy, average_loss

test_accuracy, average_test_loss = test(model, test_loader)

def inference(model, text, tokenizer):

  model.eval()
  encoded = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=12)
  input_ids = encoded['input_ids'].to(device, non_blocking=True)
  attention_mask = encoded['attention_mask'].to(device, non_blocking=True)
  with torch.inference_mode():
    outputs = model(input_ids, attention_mask)

    probabilities = torch.softmax(outputs, dim=1)
    predicted_label = torch.argmax(probabilities, dim=1)
  return predicted_label


# test_accuracy, average_loss = test(model, test_loader)
predicted_label = inference(model, "everything is great", tokenizer)
print(predicted_label)